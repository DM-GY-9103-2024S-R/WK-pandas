{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QshK8s21WBrf"
      },
      "source": [
        "# Week 06"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hf8SXUwWOho"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run the following 2 cells to import all necessary libraries and helpers for this week's exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/DM-GY-9103-2024S-R/9103-utils/raw/main/src/io_utils.py\n",
        "!wget -q https://github.com/DM-GY-9103-2024S-R/9103-utils/raw/main/src/data_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "from data_utils import MinMaxScaler, StandardScaler\n",
        "from io_utils import object_from_json_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prKGt8bzScNA"
      },
      "source": [
        "## Dataset Exploration\n",
        "\n",
        "Let's revisit the house dataset from last homework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L66UbgCWGDjF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define the location of the json file here\n",
        "HOUSES_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024S-R/9103-utils/main/datasets/json/LA_housing.json\"\n",
        "\n",
        "houses_info = object_from_json_url(HOUSES_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeWkaI_lYzIE"
      },
      "source": [
        "### Some \"light\" exploration:\n",
        "\n",
        "Ok. We should now have a list of objects with information about houses in LA.\n",
        "\n",
        "Let's work with the data to answer the following questions:\n",
        "- How many houses are in this dataset?\n",
        "- How many *features* does our dataset have?\n",
        "- What are the *features*?\n",
        "- What's the min, max and average price for the houses in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# How many houses are in the dataset?\n",
        "num_houses = len(houses_info)\n",
        "\n",
        "# Features:\n",
        "house_features = houses_info[0].keys()\n",
        "\n",
        "# Number of features:\n",
        "num_features = len(house_features)\n",
        "\n",
        "houses_info[0], house_features, num_houses, num_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37h20s-9ZLY_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# List of prices\n",
        "\n",
        "house_prices = []\n",
        "for h in houses_info:\n",
        "  house_prices.append(h[\"value\"])\n",
        "\n",
        "# min price\n",
        "min_price = min(house_prices)\n",
        "\n",
        "# max price\n",
        "max_price = max(house_prices)\n",
        "\n",
        "# avg price\n",
        "avg_price = sum(house_prices) // num_houses\n",
        "\n",
        "min_price, max_price, avg_price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### More exploring\n",
        "\n",
        "What if we wanted to get `min`, `max` and `average` values for all of the features?\n",
        "\n",
        "# ðŸ˜–\n",
        "\n",
        "Repeating the code above, can get really annoying really quick, but hopefully we can use the `Pandas` library to help.\n",
        "\n",
        "Once we load our data into a `DataFrame` we can perform many types of calculations.\n",
        "\n",
        "Here's how we load our data into a `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "houses_df = pd.DataFrame.from_records(houses_info)\n",
        "\n",
        "# And to check the first couple of rows/records\n",
        "houses_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataFrame\n",
        "\n",
        "<img src=\"./imgs/data_frame.jpg\" width=\"700px\">\n",
        "\n",
        "Now we have access to each feature by simply indexing the `DataFrame` by the feature's name.\n",
        "\n",
        "For example, to get a list of all of the prices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "house_prices = houses_df[\"value\"]\n",
        "\n",
        "# only print first 5\n",
        "house_prices.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or a list of all of the ages of the houses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "house_ages = houses_df[\"age\"]\n",
        "\n",
        "# only print first 5\n",
        "house_ages.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or, we can get a sub-section of our original data with only the `age` and `value` features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "house_ages_values = houses_df[[\"age\", \"value\"]]\n",
        "\n",
        "# only print first 5\n",
        "house_ages_values.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Doing some maths\n",
        "\n",
        "To get the smallest value of a *feature*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_price = houses_df[\"value\"].min()\n",
        "min_price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the `average` for a *feature*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "avg_price = houses_df[\"value\"].mean()\n",
        "avg_price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This gives us a list with the names of the *features*/*columns*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "house_features = list(houses_df.columns)\n",
        "house_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's similar to when we did `houses_info[0].keys()` above.\n",
        "\n",
        "Either way, we can now iterate over a list of the feature names to calculate `min`, `max` and `average` for each *feature*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for f in house_features:\n",
        "  print(f)\n",
        "  print(\"\\tmin:\", houses_df[f].min())\n",
        "  print(\"\\tmax:\", houses_df[f].max())\n",
        "  print(\"\\tavg:\", houses_df[f].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can even ask for the `min` (or `max`, or `average`) of the whole `DataFrame` and it knows to do it for each column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "houses_df.min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Average / Mean\n",
        "\n",
        "The average, or mean, value of a set of numbers is a quantity that represents the center of a collection of numbers. What this means is that we expect about half of the numbers in a collection to be higher than the mean, and the other half to be lower.\n",
        "\n",
        "The mean of a set of numbers $x_1, x_2, ..., x_n$ is calculated by dividing the sum of the values by the number of values. It's sometimes written like this:\n",
        "\n",
        "$\\displaystyle \\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i$\n",
        "\n",
        "which is the same as `sum(X) / len(X)` in Python if `X` is our list of values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standard Deviation\n",
        "\n",
        "In addition to the mean, the standard deviation is a measure of the amount of variation in a sequence of numbers.\n",
        "\n",
        "It's calculated by taking the square root of the average of the squared differences of each point to the mean of the sequence.\n",
        "\n",
        "In other words, first we calculate the difference between each point and the mean, and square this difference, then sum all of them up, divide by the number of values in the sequence, and finally take the square root:\n",
        "\n",
        "$\\displaystyle \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left(x_{i} - \\mu \\right)^{2}}$\n",
        "\n",
        "The standard deviation is a measurement of how close all of the points are to the mean.\n",
        "\n",
        "<img src=\"./imgs/std_dev.jpg\" width=\"800px\"/>\n",
        "\n",
        "Unfortunately, there is no short Python code for computing the standard deviation like there is for the average, but the `DataFrame` object has a function for computing it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mean and standard deviation for each feature\n",
        "for f in houses_df.columns:\n",
        "  print(f)\n",
        "  print(\"\\tavg:\", houses_df[f].mean())\n",
        "  print(\"\\tstd:\", houses_df[f].std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there's greater variability in the number of rooms of a house when compared to the number of bedrooms, but besides that, this isn't very useful yet because we can't really compare the standard deviations from different *features* that have different units.\n",
        "\n",
        "We'll see soon how we can use `mean` and `standard deviation` to be able to compare, combine, extrapolate values that were measured in different units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation\n",
        "\n",
        "Let's say we want to figure out if there are any *features* that correlate strongly with the house prices.\n",
        "\n",
        "This means figuring out if there are any other *features* that are a good indication for the value of a house.\n",
        "\n",
        "We only have a handful of *features*, so we can always plot them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A lot easier with Pandas DataFrames\n",
        "\n",
        "for f in house_features:\n",
        "  if f != \"value\":\n",
        "    plt.scatter(houses_df[f], houses_df[\"value\"], alpha=.1)\n",
        "    plt.xlabel(f)\n",
        "    plt.ylabel(\"value\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Covariance Matrix\n",
        "\n",
        "Instead of looking at the plots there's actually a mathematical way of calculating how much the *features* of a dataset are *related*.\n",
        "\n",
        "It's called the [covariance](https://en.wikipedia.org/wiki/Covariance), and it measures how much $2$ *features* change together.\n",
        "\n",
        "And with our `DataFrame` we can just call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "houses_df.cov()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This shows us how much each variable is related to every other variable, which will be useful when we start training models and want to reduce the amount of data we need to process, but for now we can just look at the covariances for the `value` *feature*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "houses_df.cov()[\"value\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we look at the *covariances* with the largest magnitudes, we can see that:\n",
        "- `value` correlates with `rooms`: the more rooms, the higher the price of a house.\n",
        "- `value` correlates inversely with `age`: the older the house, the less valuable it is.\n",
        "- `value` correlates somewhat with `longitude`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalizing / Standardizing / Scaling\n",
        "\n",
        "The above calculation gives us some extra information about the data, but there's a problem:\n",
        "\n",
        "All of the *features* are in different units. The range of the `rooms` *feature* is between $1$ and $17$, while the range of `age` is $2$ to $55$, and `latitude` and `longitude` vary by at most $1$ degree.\n",
        "\n",
        "Using different units to calculate the *covariance* can exaggerate how much certain *features* actually influence each other.\n",
        "\n",
        "In order to calculate *covariance* correctly we have to normalize the data in all columns to be *unitless*.\n",
        "\n",
        "One way to do this is to scale each value to be within the range $[0, 1]$ relative to the `min` and `max` values of their column.\n",
        "\n",
        "### [MinMax Scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html):\n",
        "\n",
        "<img src=\"./imgs/scaling_min_max.jpg\" width=\"700px\">\n",
        "\n",
        "The math is not very complicated, it's actually exactly like the `map()` function from `p5.js`, but we're going to use a library that will do this for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale all the values to be between 0, 1\n",
        "\n",
        "# This creates a scaler object we can use\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "# This is how we use the object on our data\n",
        "houses_min_max_df = min_max_scaler.fit_transform(houses_df)\n",
        "\n",
        "# Scaled version of DataFrame\n",
        "houses_min_max_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we can finally correctly calculate the covariances\n",
        "houses_min_max_df.cov()[\"value\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we scale our features according to the `min` and `max` value of each column, `longitude` is the top feature that correlate with `value`, followed by `rooms` and then `age`.\n",
        "\n",
        "### [Standard Scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
        "\n",
        "This is another type of *normalization* that uses the `mean` and `standard deviation` for each column to center their values at $0$ and have a range of about $[-3, 3]$.\n",
        "\n",
        "<img src=\"./imgs/scaling_std.jpg\" width=\"700px\">\n",
        "\n",
        "The math is a bit more involved, but luckily this *scaling* is also available from the [Scikit-Learn](https://scikit-learn.org) library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale all the values to be centered at 0 and (mostly) within [-3, 3]\n",
        "\n",
        "# This creates a scaler object we can use\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "# This is how we use the object on our data\n",
        "houses_std_df = std_scaler.fit_transform(houses_df)\n",
        "\n",
        "# Scaled version of DataFrame\n",
        "houses_std_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we can finally correctly calculate the covariances\n",
        "houses_std_df.cov()[\"value\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that if we scale our features according to the `mean` and `std dev` of each column, `longitude` is the top feature that correlate with `value`, followed by `rooms` and then `age`.\n",
        "\n",
        "In this case `MinMax` and `Standard` *scaling* are in agreement, but it's not always the case.\n",
        "\n",
        "Either way, *Normalizing* (or, *Standardizing*, or, *Scaling*) is an important step when working with data.\n",
        "\n",
        "This is not only true for calculating *covariances* and learning something about our datasets, but is even more important when training models with multiple *features*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One more dataset\n",
        "\n",
        "Let's load..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the location of the json file here\n",
        "DIAMONDS_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024S-R/9103-utils/main/datasets/json/diamonds.json\"\n",
        "\n",
        "# Read into Python object\n",
        "diamonds_data = object_from_json_url(DIAMONDS_FILE)\n",
        "\n",
        "# Create DataFrame\n",
        "diamonds_df = pd.DataFrame.from_records(diamonds_data)\n",
        "\n",
        "diamonds_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for f in diamonds_df.columns:\n",
        "  print(f)\n",
        "  print(\"\\tmin:\", diamonds_df[f].min())\n",
        "  print(\"\\tmax:\", diamonds_df[f].max())\n",
        "  print(\"\\tavg:\", diamonds_df[f].mean())\n",
        "  print(\"\\tstd:\", diamonds_df[f].std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ˜«\n",
        "\n",
        "Some of the *features* aren't even numerical, they're words/tags that describe some property of the diamond.\n",
        "\n",
        "We can still do some analysis. The easiest way is to just drop those columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "diamonds_numerical_df = diamonds_df.drop(columns=[\"cut\", \"color\", \"clarity\"])\n",
        "diamonds_numerical_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Now we can get some statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for f in diamonds_numerical_df.columns:\n",
        "  print(f)\n",
        "  print(\"\\tmin:\", diamonds_numerical_df[f].min())\n",
        "  print(\"\\tmax:\", diamonds_numerical_df[f].max())\n",
        "  print(\"\\tavg:\", diamonds_numerical_df[f].mean())\n",
        "  print(\"\\tstd:\", diamonds_numerical_df[f].std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¤·\n",
        "\n",
        "Which is fine if all we're doing is taking a look at our data, but once we start training models and doing more detailed analyses we'll want to use *ALL* of the data that we have available.\n",
        "\n",
        "Let's see how to work with non-numerical properties.\n",
        "\n",
        "### A Note About Feature\n",
        "\n",
        "First, let's think about and organize the different types of *features* we have seen so far.\n",
        "\n",
        "We have some *features* that are represented by *continuous* values, like: `price` or `length`.\n",
        "\n",
        "We also have *features* that are represented by *ordered* discrete values, like: `clothing size` $(S, M, L)$, or `letter grades` $(A, B, C, D, F)$.\n",
        "\n",
        "And, we can also have *features* that are represented by *UNordered* discrete values (sometimes called *categories*), like: `primary colors` $(red, green, black)$, or `states` $(PA, NY, NJ)$.\n",
        "\n",
        "<img src=\"./imgs/encoding_00.jpg\" width=\"700px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding\n",
        "\n",
        "In order to analyze datasets that have different types of *features*, it helps to be able to represent the *categorical* and *discrete* *features* in terms of numbers.\n",
        "\n",
        "This way we can perform the same types of transformations and calculations, like `mean`, `standard deviation` and `covariances`, on all of the *features* together.\n",
        "\n",
        "We'll achieve this by *encoding* these *features* using numbers. We'll create associations between the *feature* values and specific numbers, so we can easily transform these *categories* into numbers, do math, and then un-transform them back to *categories*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ordinal Encoding\n",
        "\n",
        "If the data that we want to encode has some kind of order, like with `letter grades`, where $A$ comes before $B$, which comes before $C$, and so on, we can use something called an [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html).\n",
        "\n",
        "We provide the order of the possible values of the *feature* and they will each get represented by a number in that same order:\n",
        "\n",
        "<img src=\"./imgs/encoding_ordinal.jpg\" width=\"700px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One-Hot Encoding\n",
        "\n",
        "If, however, the values we want to encode don't really have an oder, like `states`, or even `zip codes` (these are numbers, but it's not like we have a good reason for considering $10001$ *smaller* than $15201$), we can use an [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n",
        "\n",
        "This encoder turns each of the possible values of a *feature* into its own *feature*, and uses the numbers $0$ and $1$ to mark whether a *record* had that value or not:\n",
        "\n",
        "<img src=\"./imgs/encoding_one_hot.jpg\" width=\"700px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding Diamonds\n",
        "\n",
        "If we look at the [dataset info](https://www.kaggle.com/datasets/shivam2503/diamonds) for our diamond data, we'll see that all three of our *discrete* *features* have an order:\n",
        "\n",
        "<img src=\"./imgs/diamonds.jpg\" width=\"700px\"/>\n",
        "\n",
        "They all have a *worse* and a *best* option.\n",
        "\n",
        "This means we can use an [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) to assign numbers to them and include them back into our dataset analysis and eventually use them for training models.\n",
        "\n",
        "To use the encoder, we first we have to determine all the possible values of each of these features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for f in [\"cut\", \"color\", \"clarity\"]:\n",
        "  print(diamonds_df[f].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then re-order them from worst to best:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cut_order = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\n",
        "color_order = ['J', 'I', 'H', 'G', 'F', 'E', 'D']\n",
        "clarity_order = ['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the encoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "diamond_encoder = OrdinalEncoder(categories=[cut_order, color_order, clarity_order])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And apply it to our `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode the columns\n",
        "ccc_vals = diamond_encoder.fit_transform(diamonds_df[[\"cut\", \"color\", \"clarity\"]].values)\n",
        "\n",
        "# Put the values back in the original DataFrame\n",
        "diamonds_df[[\"cut\", \"color\", \"clarity\"]] = ccc_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can rerun our loop to get statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for f in diamonds_df.columns:\n",
        "  print(f)\n",
        "  print(\"\\tmin:\", diamonds_df[f].min())\n",
        "  print(\"\\tmax:\", diamonds_df[f].max())\n",
        "  print(\"\\tavg:\", diamonds_df[f].mean())\n",
        "  print(\"\\tstd:\", diamonds_df[f].std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalize the data, get covariances and plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale all the values to be between 0, 1\n",
        "\n",
        "# This creates a scaler object we can use\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "# This is how we use the object on our data\n",
        "diamonds_min_max_df = min_max_scaler.fit_transform(diamonds_df)\n",
        "\n",
        "# Scaled version of DataFrame\n",
        "diamonds_min_max_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "diamonds_min_max_df.cov()[\"price\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the original data\n",
        "for f in diamonds_df.columns:\n",
        "  if f != \"price\":\n",
        "    plt.scatter(diamonds_df[f], diamonds_df[\"price\"], alpha=.1)\n",
        "    plt.xlabel(f)\n",
        "    plt.ylabel(\"price\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next time: Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "from data_utils import LinearRegression, MinMaxScaler\n",
        "from io_utils import object_from_json_url\n",
        "\n",
        "# Load Dataset\n",
        "DIAMONDS_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024S-R/9103-utils/main/datasets/json/diamonds.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "diamonds_data = object_from_json_url(DIAMONDS_FILE)\n",
        "diamonds_df = pd.DataFrame.from_records(diamonds_data)\n",
        "\n",
        "\n",
        "# Encode\n",
        "cut_order = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\n",
        "color_order = ['J', 'I', 'H', 'G', 'F', 'E', 'D']\n",
        "clarity_order = ['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF']\n",
        "\n",
        "diamond_encoder = OrdinalEncoder(categories=[cut_order, color_order, clarity_order])\n",
        "\n",
        "ccc_vals = diamond_encoder.fit_transform(diamonds_df[[\"cut\", \"color\", \"clarity\"]].values)\n",
        "diamonds_df[[\"cut\", \"color\", \"clarity\"]] = ccc_vals\n",
        "\n",
        "\n",
        "# Normalize\n",
        "diamond_scaler = MinMaxScaler()\n",
        "diamonds_scaled = diamond_scaler.fit_transform(diamonds_df)\n",
        "\n",
        "\n",
        "# Linear Regression\n",
        "price_model = LinearRegression()\n",
        "\n",
        "# Get prices and features\n",
        "prices = diamonds_scaled[\"price\"]\n",
        "features = diamonds_scaled.drop(columns=[\"price\"])\n",
        "\n",
        "# Build the model\n",
        "price_model.fit(features, prices)\n",
        "\n",
        "# Run the model on the training data\n",
        "predicted_scaled = price_model.predict(features)\n",
        "\n",
        "\n",
        "# Un-normalize the data\n",
        "predicted = diamond_scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "\n",
        "# Measure Model error\n",
        "price_mse = mean_squared_error(diamonds_df[\"price\"].values, predicted[\"price\"].values, squared=False)\n",
        "print(price_mse)\n",
        "\n",
        "\n",
        "# Plot predictions\n",
        "carats = diamonds_scaled[[\"carat\"]]\n",
        "\n",
        "plt.plot(sorted(prices), marker='o', markersize=8)\n",
        "plt.plot(sorted(predicted_scaled[\"price\"]), marker='o', markersize=2, linestyle='', color='r')\n",
        "plt.title(\"sorted prices\")\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(carats, prices, marker='o', linestyle='', alpha=0.3)\n",
        "plt.scatter(carats, predicted_scaled, color='r', marker='o', linestyle='', alpha=0.1)\n",
        "plt.xlabel(\"carat\")\n",
        "plt.ylabel(\"price\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.17 ('hf-model')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
